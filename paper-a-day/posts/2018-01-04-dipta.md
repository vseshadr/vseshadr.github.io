title: Summary of Near-Memory Address Translation
---

Title: Near-Memory Address Translation
Venue: PACT 2017

## Problem

In the virtual memory framework in today's systems, a virtual page can
be mapped to any physical frame. Such fully-associative mapping was
necessary in early days owing to the high incidence and cost of page
faults. However, in modern systems, page faults are rare. However, the
fully-associative mapping incurs high virtual-to-physical memory
translation overhead.


## Idea

The paper shows that there is negligible increase in page faults when
moving from a fully-associative mapping to a set-associative mapping
with small associativity (1, 2, 4, or 8). With a set-associative
mapping between virtual and physical memory, the paper uses an
inverted index to lookup data in physical memory. In other words, a
virtual page can be mapped to only one of few slots. With this
approach, each physical frame can be associated with a tag of the
virtual page that is mapped to the frame. To make virtual page tags
unique across processes, the paper includes a 12-bit address space ID
as part of the tag. The paper proposes a few optimizations to work
with constraints in existing systems.


## Strengths

1. Virtual memory as we know it needs a serious rethinking, especially
with data being shared across multiple heterogeneous compute
devices. This paper simplifies virtual memory design, thereby reducing
the translation overhead. It is a step in the right direction.

2. The paper is well written and organized.


## Weaknesses

1. The proposal may definitely make the OS memory management complex.

1. The paper punts on some issues like synonyms.

2. In retrospect, the abstract problem boils down to a cache
organization problem, and many of the ideas are explored as part of
prior 3D-stacked DRAM caching works.

3. Frequent use of the word novel :-)


## Alternate Summary of the Paper

Here is an alternate way of thinking about the paper's proposal. This
thought was only in retrospect, therefore not intended to trivialize
the original proposal.

With a 12-bit address space ID and a 48-bit virtual address space, we
have a 60-bit global virtual address space. If the cache line size is
64 bytes, this global address space has 2^54 cache lines. We can now
think of our physical memory as a cache for this global address
space. A 2 GB main memory will hold 2^25 cache lines. If we use main
memory as a direct mapped cache, then we need a 31-bit tag for each
entry. 

Now, we can reduce the 31-bit overhead by increasing the cache line
size. If we make the cache line size as 4 KB, it is equivalent to the
initial proposal of the paper. The different proposals and
optimizations (except for the offset based organization) proposed in
the paper can be fit in this general space by varying the cache line
size and the choice of index bits.